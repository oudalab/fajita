{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 52min 49s, sys: 33min 51s, total: 3h 26min 41s\n",
      "Wall time: 2h 50min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pymongo import MongoClient\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "client=MongoClient()\n",
    "client=MongoClient('mongodb://localhost:/')\n",
    "db=client['eventData']\n",
    "sen=db.documents_english\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "texts = []\n",
    "docIds=[]\n",
    "actuallyTrained=0;\n",
    "for i in sen.find():\n",
    "    try:\n",
    "        raw = ''.join(i['document']).lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        texts.append(stemmed_tokens)\n",
    "        docIds.append(i['_id'])\n",
    "        actuallyTrained=actuallyTrained+1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=1)\n",
    "\n",
    "dim=20 \n",
    "result=[]\n",
    "for i in range(0,actuallyTrained):\n",
    "    feature=[]\n",
    "    previousindex=0\n",
    "    for item in ldamodel[corpus[i]]:\n",
    "        index=item[0]\n",
    "        #print(index)\n",
    "        for beforeindex in range(previousindex,index):\n",
    "            feature.append(0)\n",
    "        feature.append(item[1])\n",
    "        previousindex=index+1\n",
    "    while (len(feature)<dim):\n",
    "        feature.append(0);  #add in 0 at the end\n",
    "    result.append(feature)\n",
    "    \n",
    "kmeanstest=np.array(result)\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(kmeanstest)\n",
    "\n",
    "#and before building  the dictionary test if the size of docIds and cluster result dimensions are the same.\n",
    "try:\n",
    "    assert(len(docIds)==kmeans.labels_.size)\n",
    "    dictionary_cocId_topicClusterItBelongs={}\n",
    "    for i in range(0,actuallyTrained):\n",
    "        dictionary_cocId_topicClusterItBelongs.update({docIds[i]:kmeans.labels_[i]})\n",
    "except:\n",
    "    print(\"the docIds size is different from the topic # cluster size\")\n",
    "\n",
    "with open('traingrst_english.pkl', 'wb') as output:\n",
    "    pickle.dump(dictionary_cocId_topicClusterItBelongs,output)\n",
    "\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161388\n",
      "CPU times: user 3min 32s, sys: 24 s, total: 3min 56s\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dic=pickle.load(open('traingrst_english.pkl', 'rb'))\n",
    "topic_set=0;\n",
    "for i in sen.find(modifiers={\"$snapshot\": True}):\n",
    "    try:\n",
    "        #find the topic from the dictionary\n",
    "        docid=i['_id']\n",
    "        topic=dic[docid]\n",
    "        #print(topic)\n",
    "        sen.update_many(\n",
    "        {\"_id\": str(docid)},\n",
    "        {\n",
    "        \"$set\": {\n",
    "            #this need to cast to int, otherwise it has a can't encode object error after it use pickle to load.\n",
    "            \"topic\":int(topic)\n",
    "        }\n",
    "        }\n",
    "        )\n",
    "        topic_set=topic_set+1\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        pass\n",
    "print(topic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
